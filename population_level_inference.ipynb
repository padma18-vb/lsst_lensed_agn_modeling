{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6513f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED IMPORTS\n",
    "# FILE MANAGEMENT\n",
    "import h5py\n",
    "import os\n",
    "from IPython.utils import io\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# DATA MANIPULATION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# STATISTICS\n",
    "from scipy.stats import norm, truncnorm, uniform, beta, multivariate_normal\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import simple_norm\n",
    "import matplotlib.colors as mpc\n",
    "import corner\n",
    "from matplotlib.patches import Patch\n",
    "import astropy.visualization as asviz\n",
    "\n",
    "\n",
    "SMALL_SIZE = 17\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 30\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)    # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params =np.array( ['main_deflector_parameters_theta_E', 'main_deflector_parameters_gamma1',\n",
    "                   'main_deflector_parameters_gamma2', 'main_deflector_parameters_gamma', \n",
    "                   'main_deflector_parameters_e1', 'main_deflector_parameters_e2', 'main_deflector_parameters_center_x',\n",
    "                  'main_deflector_parameters_center_y', 'source_parameters_R_sersic', 'source_parameters_mag_app'])\n",
    "\n",
    "labels = np.array([\"$\\\\theta_E$\", \"$\\gamma_1$\", \"$\\gamma_2$\", \"$\\gamma_{lens}$\", \"$e_1$\", \"$e_2$\",\n",
    "                   '$x_D$', '$y_D$', '$R_{src}$', \"$m_{i}$\"])\n",
    "mu_labels = np.array(['$\\mathcal{M}(' + i[1:-1] + ')$' for i in np.array(labels)])\n",
    "std_labels = np.array(['$\\Sigma(' + i[1:-1] + ')$' for i in np.array(labels)])\n",
    "\n",
    "labels_dict = dict(zip(learning_params, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cfe7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### latils imports\n",
    "from latils import prepRes, make_contour , get_train_data,  get_obj_of_wide_posteriors_obj, retrieve_chains_h5\n",
    "\n",
    "# from lenstronomy.Util.param_util import ellipticity2phi_q, shear_cartesian2polar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24baa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### fiducial preparation\n",
    "ALobj = prepRes('full', 'all',8,learning_params[:8],'All Light Included','rebeccapurple','0118/all','0325/all_no_single','full/0118/all', 'data/0325_results',mode_of_stopping='early_stopping')\n",
    "\n",
    "### orange dataset: applied network trained with mass-light correlated to test with different mass-light ellipticity correlations\n",
    "ALobj_dinos_ml = prepRes('full', 'all',8,learning_params[:8],'All Light Included','rebeccapurple','0118/all','0429/all_dinos_ml','full/0118/all', 'data/dinos_qm_ql',mode_of_stopping='early_stopping')\n",
    "\n",
    "NLobj = prepRes('full', 'nolens',8,learning_params[:8],'Lens Light Subtracted','mediumaquamarine','0118/nolens','0325/nolens','full/0118/nolens', 'data/0325_results',mode_of_stopping='early_stopping')\n",
    "\n",
    "NSobj = prepRes('full', 'nosrc',10,learning_params[:10],'Lens+AGN Light Subtracted','gold','0118/nosrc','0325/nosrc','full/0118/nosrc', 'data/0325_results',mode_of_stopping='early_stopping')\n",
    "\n",
    "### trained without correlations\n",
    "nc_ALobj = prepRes('full', 'nc_all_om10',8,learning_params[:8],'NC:All Light Included','darkgoldenrod','0310/all','0325/all_no_single','full/0310/all', 'data/0325_train_no_corr',mode_of_stopping='early_stopping')\n",
    "nc_NLobj = prepRes('full', 'nc_nolens_om10',8,learning_params[:8],'NC:Lens Light Subtracted','rebeccapurple','0310/nolens','0325/nolens','full/0310/nolens', 'data/0325_train_no_corr',mode_of_stopping='early_stopping')\n",
    "\n",
    "### valid_results\n",
    "ALvobj = prepRes('full', 'valid_all',8,learning_params[:8],'V: All Light Included','navajowhite','0118/all','0118/valid_all','full/0118/all', 'data/0118_valid_results',mode_of_stopping='early_stopping')\n",
    "NLvobj = prepRes('full', 'valid_nolens',8,learning_params[:8],'V: Lens Light Subtracted','aquamarine','0118/nolens','0118/valid_nolens','full/0118/nolens', 'data/0118_valid_results',mode_of_stopping='early_stopping')\n",
    "\n",
    "nc_ALvobj = prepRes('full', 'nc_all',8,learning_params[:8],'NC:All Light Included','darkgoldenrod','0310/all','0118/valid_all','full/0310/all', 'data/0320_results_held_out',mode_of_stopping='early_stopping')\n",
    "nc_NLvobj = prepRes('full', 'nc_nolens',8,learning_params[:8],'NC:Lens Light Subtracted','rebeccapurple','0310/nolens','0118/valid_nolens','full/0310/nolens', 'data/0320_results_held_out',mode_of_stopping='early_stopping')\n",
    "\n",
    "### diag_results\n",
    "ALdobj = prepRes('diag', 'all_diag',8,learning_params[:8],'D: All Light Included','tan','0118/all','0325/all_no_single','diagonal/0118/all', 'data/0118_diag_results',mode_of_stopping='early_stopping')\n",
    "\n",
    "### bright_results\n",
    "ALbobj = prepRes('full', 'allb',8,learning_params[:8],'All Light Included','gold','0118/all','0118/all/bright','full/0118/all', 'data/0118_bright_results',mode_of_stopping='early_stopping')\n",
    "NLbobj = prepRes('full', 'nolensb',8,learning_params[:8],'Lens Light Subtracted','mediumaquamarine','0118/nolens','0118/nolens/bright','full/0118/nolens', 'data/0118_bright_results',mode_of_stopping='early_stopping')\n",
    "NSbobj = prepRes('full', 'nosrcb',10,learning_params[:10],'Lens+AGN Light Subtracted','rebeccapurple','0118/nosrc','0118/nosrc/bright','full/0118/nosrc', 'data/0118_bright_results',mode_of_stopping='early_stopping')\n",
    "\n",
    "\n",
    "### deconv_results\n",
    "ALdeobj = prepRes('full', 'all',10,learning_params[:10],'EM-D: All Light Included','gold','0118/deconvolved/all','0325/deconvolved/all','full/0118/deconvolved/all', 'data/0325_deconv_results',mode_of_stopping='early_stopping')\n",
    "NLdeobj = prepRes('full', 'nolens',10,learning_params[:10],'EM-D: Lens Light Subtracted','mediumaquamarine','0118/deconvolved/nolens','0325/deconvolved/nolens','full/0118/deconvolved/nolens', 'data/0325_deconv_results',mode_of_stopping='early_stopping')\n",
    "NSdeobj = prepRes('full', 'nosrc',10,learning_params[:10],'EM-D: Lens+AGN Light Subtracted','mediumpurple','0118/deconvolved/nosrc','0325/deconvolved/nosrc','full/0118/deconvolved/nosrc', 'data/0325_deconv_results',mode_of_stopping='early_stopping')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = np.array([ALobj,ALobj_dinos_ml, NLobj, ALbobj, NLbobj, NSbobj, ALdeobj, NLdeobj, NSdeobj, nc_ALobj, nc_NLobj, ALvobj, NLvobj])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ef025",
   "metadata": {},
   "source": [
    "### Figure 7: Population-level parameter recovery [fiducial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c859f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypermodel_plot(results_df, preps,n_params_learned, h5_files, params,colors, categories,burnin=3000, save_name=None,\n",
    "                    show_corr = False,obj_list=None):\n",
    "    num_preps = len(preps)\n",
    "    # if len(np.unique(preps)) ==1:\n",
    "    #     results_df = results_df.loc[preps]\n",
    "    # else:\n",
    "    #     results_df = results_df.loc[np.unique(preps)]\n",
    "    legend_elements = []\n",
    "    h5i = 0\n",
    "    list_of_dists = []\n",
    "    # list_of_dists_std= []\n",
    "    mu_labels = np.array(['$\\mathcal{M}(' + '{' + i[1:-1] + '})$' for i in np.array(labels)[params]])\n",
    "    std_labels = np.array(['$\\Sigma(' + '{' + i[1:-1] + '})$' for i in np.array(labels)[params]])\n",
    "    all_labels = np.append(mu_labels, std_labels)\n",
    "    truths, errors, err_perc = [],[],[]\n",
    "    # decide looping factors\n",
    "    for prep in preps:\n",
    "        df = results_df[h5i]\n",
    "        # print(prep)\n",
    "        # print(results_df.loc[prep,'learning_params'])\n",
    "        # print(np.array(results_df.loc[prep,'learning_params']))\n",
    "        learning_params=np.array(df.loc[prep,'learning_params'])[params]\n",
    "        category = df.loc[prep, 'name']\n",
    "        color = df.loc[prep, 'color']\n",
    "        legend_elements.append(Patch(facecolor=color, edgecolor=color, label=category))\n",
    "        \n",
    "        test_data = pd.read_csv(os.path.join(df.loc[prep, 'path_to_test_images'], 'metadata.csv'))\n",
    "        train_data = get_train_data(df, prep)[learning_params]\n",
    "        mu_train = np.array(train_data.loc[:, learning_params].mean())\n",
    "        std_train = np.array(train_data.loc[:, learning_params].std())\n",
    "        print('mu_train:', mu_train)\n",
    "        print('std_train:', std_train)\n",
    "        n_lenses = len(test_data)\n",
    "        bad_ind = get_obj_of_wide_posteriors_obj(obj_list[h5i])\n",
    "        good_ind = [i for i in range(n_lenses) if i not in bad_ind]\n",
    "        mu_test = np.array(test_data.loc[good_ind, learning_params].mean())\n",
    "        print('mu_test before:', np.array(test_data.loc[:, learning_params].mean()))\n",
    "        print('mu_test after:',mu_test)\n",
    "        cov_test = np.array(test_data.loc[good_ind, learning_params].cov())\n",
    "        std_test = np.array(test_data.loc[good_ind, learning_params].std())\n",
    "        print('std_test before:',np.array(test_data.loc[:, learning_params].std()))\n",
    "        print('mu_test after:',std_test)\n",
    "        emcee_chain = retrieve_chains_h5(h5_files[h5i])\n",
    "        all_params = np.append(params, params+n_params_learned[h5i])\n",
    "        chain_full = emcee_chain[:, burnin:, all_params]\n",
    "        how_much_learned = df.loc[prep,'n_params']\n",
    "        print(emcee_chain.shape)\n",
    "        good_walk = []\n",
    "        \n",
    "        for i in range(40):\n",
    "            for n in range(chain_full.shape[2]):\n",
    "                arr = chain_full[i, :, n].T\n",
    "                arr2 = chain_full[i, :, n].T\n",
    "                if arr[0]==arr[-1] or arr2[0]==arr2[-1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    good_walk.append(i)\n",
    "        good_walk = np.unique(good_walk)\n",
    "        print(prep, len(good_walk))\n",
    "        chain_full = chain_full[np.array(good_walk), :, :]\n",
    "        reshaped = chain_full.reshape(-1,chain_full.shape[-1])\n",
    "        list_of_dists.append(reshaped)\n",
    "        truths_arr = np.append(mu_test, std_test)\n",
    "        truths.append(truths_arr)\n",
    "        errors_arr = (reshaped.mean(axis=0) - truths_arr)\n",
    "        err_perc_arr = (reshaped.mean(axis=0) - truths_arr)*100/truths_arr\n",
    "        errors.append(errors_arr)\n",
    "        err_perc.append(err_perc_arr)\n",
    "        h5i +=1\n",
    "    # print(truths)\n",
    "    print(colors)\n",
    "    # truths=[truths[0]]*num_preps\n",
    "    return mu_test, std_test, mu_train, std_train, errors, err_perc, make_contour(list_of_dists, all_labels, categories, colors, truths_list=[truths[0], truths[0]],show_correlation=show_corr,save_fig=save_name)\n",
    "    # make_contour(list_of_dists_std, std_labels, categories, colors, truths_list=[std_test]*num_preps, save_fig='sigma_'+save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ad8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_files = [f'{NLobj.prep}_1300_obj.h5',f'{NLobj.prep}_no_x_y_r_m.h5',]\n",
    "params = [0]\n",
    "\n",
    "select_obj = obj_list[params]\n",
    "# h5_files = [o.h5_file for o in obj_list[params]]\n",
    "h5_files = ['data/all_lsst_0325_uniform.h5']\n",
    "# colors = ['mediumaquamarine', 'darkgreen']\n",
    "# colors = ['gold', 'red']\n",
    "colors=['rebeccapurple']\n",
    "# colors[-3:] = ['red','blue','orange']\n",
    "preps = [o.prep for o in select_obj]\n",
    "# preps=['nolens','nolens']\n",
    "# categories =[o.name for o in obj_list[params]]\n",
    "# categories = ['Uniform','Mass traces Light','Bright Hosts','No Distribution Shift','Informative Prior',]\n",
    "categories = [o.name for o in select_obj]\n",
    "# categories = ['All Light Included: $q_l$ < $q_m$', 'All Light Included: $q_l$ = $q_m$']\n",
    "# colors.insert(0, 'lightblue')\n",
    "# colors.insert(1, 'salmon')\n",
    "# categories = [f\"Sample {i} from True Dist\" for i in range(1,6)]\n",
    "# categories.insert(0, \"Unbiased Recovery - \\nTrue Dist Mean\")\n",
    "# categories.insert(1, 'Paltas Recovery')\n",
    "mu_test, std_test, mu_train, std_train, errors, err_perc, fig = hypermodel_plot([o.df for o in obj_list[params]], \n",
    "                      n_params_learned=[8],\n",
    "              preps=preps,\n",
    "              h5_files=h5_files,\n",
    "              params=np.array([0,3,4,1]),colors = colors, categories=categories,burnin=3000,\n",
    "             save_name=None, obj_list=obj_list[params]);\n",
    "# fig.tight_layout()\n",
    "axs = fig.axes \n",
    "axs = np.array(axs)\n",
    "for ax in axs:\n",
    "        for spine in ax.spines.values():\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "axs = axs.reshape(8,8)\n",
    "\n",
    "nrows, ncols = axs.shape\n",
    "\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        xlabel = ax.get_xlabel()\n",
    "        ylabel = ax.get_ylabel()\n",
    "        ax.set_ylabel(ylabel, x=-0.4)\n",
    "        ax.set_xlabel(xlabel, y=-0.4)\n",
    "\n",
    "        # Left column → y ticks\n",
    "        if col == 0:\n",
    "            ax.minorticks_on()\n",
    "            ax.tick_params(axis='y', which='both', direction='out', length=6, width=1)\n",
    "        else:\n",
    "            ax.tick_params(labelleft=False, left=False)\n",
    "\n",
    "        # Bottom row → x ticks\n",
    "        if row == nrows - 1:\n",
    "            ax.minorticks_on()\n",
    "            ax.tick_params(axis='x', which='both', direction='out', length=6, width=1)\n",
    "        else:\n",
    "            ax.tick_params(labelbottom=False, bottom=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf4509",
   "metadata": {},
   "source": [
    "### Figure 9: Inference of $\\rm \\gamma_{lens}$ Population Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ace9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in obj_list:\n",
    "    print(obj.name, obj.y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the chains just created\n",
    "\n",
    "all_lsst = retrieve_chains_h5('data/all_lsst_0325_uniform.h5')\n",
    "nolens_lsst = retrieve_chains_h5('data/nolens_lsst_0325_uniform.h5')\n",
    "\n",
    "all_lsst=all_lsst[:, 3000:, :].reshape(-1,16)\n",
    "nolens_lsst=nolens_lsst[:, 3000:, :].reshape(-1,16)\n",
    "\n",
    "all_bright = retrieve_chains_h5('data/all_0327_bright_uniform.h5')\n",
    "all_bright=all_bright[:, 3000:, :].reshape(-1,16)\n",
    "nolens_bright = retrieve_chains_h5('data/nolens_0327_bright_uniform.h5')\n",
    "nolens_bright=nolens_bright[:, 3000:, :].reshape(-1,16)\n",
    "nosrc_bright = retrieve_chains_h5('data/nosrc_0327_bright_uniform.h5')\n",
    "nosrc_bright=nosrc_bright[:, 3000:, :].reshape(-1,16)\n",
    "\n",
    "all_deconv = retrieve_chains_h5('data/all_0325_uniform.h5')\n",
    "nolens_deconv = retrieve_chains_h5('data/nolens_0325_uniform.h5')\n",
    "nosrc_deconv = retrieve_chains_h5('data/nosrc_0325_uniform.h5')\n",
    "all_deconv=all_deconv[:, 3000:, :].reshape(-1,16)\n",
    "nolens_deconv=nolens_deconv[:, 3000:, :].reshape(-1,16)\n",
    "nosrc_deconv=nosrc_deconv[:, 3000:, :].reshape(-1,16)\n",
    "\n",
    "\n",
    "nc_all = retrieve_chains_h5('data/nc_all_om10_0325_uniform.h5')\n",
    "nc_nolens = retrieve_chains_h5('data/nc_nolens_om10_0325_uniform.h5')\n",
    "nc_all=nc_all[:, 3000:, :].reshape(-1,16)\n",
    "nc_nolens=nc_nolens[:, 3000:, :].reshape(-1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00eb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_chain = np.array([all_lsst, nolens_lsst,all_bright, nolens_bright, nosrc_bright,  all_deconv, nolens_deconv, nosrc_deconv, nc_all, nc_nolens])\n",
    "corresponding_obj_list = np.array([ALobj, NLobj, ALbobj, NLbobj, NSbobj, ALdeobj, NLdeobj, NSdeobj, nc_ALobj, nc_NLobj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mean_std_pop(obj, params):\n",
    "    vec = np.array([])\n",
    "    bad_obj = get_obj_of_wide_posteriors_obj(ALobj, params=np.arange(8))\n",
    "    good_obj = [i for i in range(obj.num_obj) if i not in bad_obj]\n",
    "    if type(params)!=list:\n",
    "        params = [params]\n",
    "    for p in params:\n",
    "        vec = np.append(vec, obj.y_test[good_obj, p].mean())\n",
    "        vec = np.append(vec, obj.y_test[good_obj, p].std())\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_param_recovery_all_preps(param):\n",
    "param = 3\n",
    "medians = np.median(list_of_chain, axis=1)[:, param].flatten()\n",
    "scatters = np.std(list_of_chain,axis=1)[:, param].flatten()*2\n",
    "all_means_scatters = np.array([return_mean_std_pop(o, param) for o in corresponding_obj_list]).T\n",
    "true_means, true_scatters = all_means_scatters\n",
    "\n",
    "# bad_dinos_obj = get_obj_of_wide_posteriors_obj(ALobj_dinos_ml)\n",
    "# y_test_filtered_dinos = np.delete(ALobj_dinos_ml.y_test, bad_dinos_obj, axis=0)\n",
    "# true_dinos_mean = np.mean(ALobj_dinos_ml.y_test[:, param],axis=0)\n",
    "# true_dinos_scatter = np.std(ALobj_dinos_ml.y_test[:, param],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# make a plot showing the deviation of the median of the chains from the true hyperparameters\n",
    "\n",
    "# scatters = np.array([true_scatter, true_scatter, true_bright_scatter, true_bright_scatter, true_bright_scatter, true_scatter, true_scatter, true_scatter, true_scatter, true_scatter])\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "colors=np.array(['rebeccapurple', 'mediumturquoise', 'rebeccapurple','mediumturquoise','red','rebeccapurple','mediumturquoise','red','rebeccapurple','mediumturquoise'])\n",
    "ax.errorbar(np.arange(len(list_of_chain)), medians-true_means,yerr=scatters, fmt='none', ecolor='black',capsize=5)\n",
    "\n",
    "ax.scatter(np.arange(len(list_of_chain)), medians-true_means,color=colors,s=80)\n",
    "# ax.set_xticks(np.arange(len(list_of_chain)))\n",
    "ax.axhline(0, color='k', lw='4', ls='--')\n",
    "ax.axvline(1.5, color='gray', lw='2', ls='--')\n",
    "ax.axvline(4.5, color='gray', lw='2', ls='--')\n",
    "ax.axvline(7.5, color='gray', lw='2', ls='--')\n",
    "\n",
    "ax.set_xticks([]);\n",
    "# ax.text(0.05,-0.1, \"Fiducial\",transform=ax.transAxes)\n",
    "ax.text(0.05, -0.08, r\"$\\mathbf{Fiducial}$\", transform=ax.transAxes)\n",
    "ax.text(0.35, -0.12, \"Bright Host \\nGalaxies\", ha='center',transform=ax.transAxes)\n",
    "\n",
    "ax.text(0.65, -0.09, \"Deconvolved\", ha='center',transform=ax.transAxes)\n",
    "\n",
    "\n",
    "ax.text(0.93, -0.12, \"Trained without\\nMass Light Correlations\", ha='center', transform=ax.transAxes)\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='All Light Included', markerfacecolor='rebeccapurple', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Lens Light Subtracted', markerfacecolor='mediumturquoise', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Lens and AGN Light Subtracted', markerfacecolor='red', markersize=10)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper center',  frameon=True, facecolor='white', framealpha=1)\n",
    "ax.set_ylabel(r\"$\\Delta(\\mathcal{M}\" + f'({labels[param]})' + \")$\")\n",
    "ax.minorticks_on()\n",
    "ax.tick_params(which='minor', length=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912a4f7",
   "metadata": {},
   "source": [
    "### Figure 10: Comparing the inferred cPDF against the true underlying distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea24492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenstronomy.Util import param_util\n",
    "def get_shear(param_array):\n",
    "    # returns phi, gamma\n",
    "    return param_util.shear_cartesian2polar(param_array[:, 1], param_array[:, 2])\n",
    "\n",
    "def get_ellip(param_array):\n",
    "    # return phi, q\n",
    "    return param_util.ellipticity2phi_q(param_array[:, 4], param_array[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import scipy\n",
    "import copy\n",
    "# shear_cartesian2polar,ellipticity2phi_q\n",
    "def plot_interim_conditional(obj_list, pi=np.array([0,1,2,3,4,5]), pi_to_plot = [0, 3, 1, 4],plot_true_dist = True,plot_q_gamma = False):\n",
    "    obj = obj_list[0]\n",
    "    labels_new = labels.copy()\n",
    "    learning_params =np.array( ['main_deflector_parameters_theta_E', 'main_deflector_parameters_gamma1',\n",
    "                   'main_deflector_parameters_gamma2', 'main_deflector_parameters_gamma', \n",
    "                   'main_deflector_parameters_e1', 'main_deflector_parameters_e2', 'main_deflector_parameters_center_x',\n",
    "                  'main_deflector_parameters_center_y', 'source_parameters_R_sersic', 'source_parameters_mag_app'])\n",
    "\n",
    "    learning_params = learning_params[:obj.num_param]\n",
    "    all_train = get_train_data(obj.df, obj.prep)\n",
    "    y_test = copy.deepcopy(obj.y_test)\n",
    "\n",
    "    if plot_q_gamma:\n",
    "        phi_e, q = param_util.ellipticity2phi_q(np.array(all_train['main_deflector_parameters_e1']), np.array(all_train['main_deflector_parameters_e2']))\n",
    "        all_train['main_deflector_parameters_q'] = q\n",
    "        all_train['main_deflector_parameters_phi_e'] = phi_e\n",
    "\n",
    "        phi_g, gamma_ext = param_util.shear_cartesian2polar(np.array(all_train['main_deflector_parameters_gamma1']), np.array(all_train['main_deflector_parameters_gamma2']))\n",
    "        all_train['main_deflector_parameters_gamma_ext'] = gamma_ext\n",
    "        all_train['main_deflector_parameters_phi_g'] = phi_g\n",
    "        learning_params = np.array(['main_deflector_parameters_theta_E', 'main_deflector_parameters_gamma_ext','main_deflector_parameters_phi_g',\n",
    "        'main_deflector_parameters_gamma', 'main_deflector_parameters_q', 'main_deflector_parameters_phi_e','main_deflector_parameters_center_x',\n",
    "                  'main_deflector_parameters_center_y', 'source_parameters_R_sersic', 'source_parameters_mag_app'])\n",
    "        \n",
    "        labels_new[1] = '$\\gamma_{ext}$'\n",
    "        labels_new[2] = '$\\phi_{\\gamma}$'\n",
    "        labels_new[4] = 'q'\n",
    "        labels_new[5] = '$\\phi_{q}$'\n",
    "        phi_e_test, q_test = get_ellip(y_test)\n",
    "        phi_g_test, gamma_ext_test =get_shear(y_test)\n",
    "        y_test[:, 1], y_test[:, 2], y_test[:, 4], y_test[:, 5] = gamma_ext_test, phi_g_test, q_test, phi_e_test\n",
    "    print(learning_params[pi])\n",
    "    train_mean = np.mean(all_train[learning_params[pi]], axis=0)\n",
    "    train_scatter = np.std(all_train[learning_params[pi]], axis=0)\n",
    "    cov = np.zeros((len(pi), len(pi)))\n",
    "    cov = np.fill_diagonal(cov,train_scatter[pi])\n",
    "    mu_test = y_test.mean(axis=0)\n",
    "    sigma_test = y_test.std(axis=0)\n",
    "    if plot_true_dist:\n",
    "        list_of_dists = [all_train[learning_params[pi]], y_test[:, pi]]\n",
    "    else:\n",
    "        list_of_dists = [all_train[learning_params[pi]]]\n",
    "    # list_of_dists = [multivariate_normal(train_mean[pi],train_scatter[pi]**2).rvs(5000),\n",
    "    #                         multivariate_normal(mu_test[pi],sigma_test[pi]**2).rvs(5000)]\n",
    "    test_cov = scipy.stats.Covariance.from_diagonal(sigma_test[pi]**2).covariance\n",
    "    train_cov = scipy.stats.Covariance.from_diagonal(train_scatter[pi]**2).covariance\n",
    "\n",
    "\n",
    "    # print(a.covariance, sigma_test[pi]**2)\n",
    "    \n",
    "    # list_of_dists = [all_train[learning_params[pi]], y_test[:, pi]]\n",
    "    if plot_true_dist:\n",
    "        categories=['Interim Prior', \"True Population\"]\n",
    "        colors=['gray','forestgreen']\n",
    "    else:\n",
    "        categories=['Training distribution: ' + r'$\\rm p(\\xi_k|\\nu_{int})$: ' + 'samples \\nfrom prior assumed for NPE posteriors']\n",
    "        colors=['gray']\n",
    "    if plot_true_dist:\n",
    "        truths_list=[train_mean[pi], mu_test[pi]]\n",
    "    else:\n",
    "        truths_list = [train_mean[pi]]\n",
    "    for obj in obj_list:\n",
    "        all_chain = retrieve_chains_h5(obj.h5_file)\n",
    "        all_chain = all_chain[:, 3000:, :].reshape(-1, obj.num_param*2)\n",
    "        cpdf_mu = np.median(all_chain[:, :obj.num_param], axis=0)\n",
    "        cpdf_sigma = np.median(all_chain[:, obj.num_param:], axis=0)\n",
    "        hypermodel = multivariate_normal(cpdf_mu[pi], cpdf_sigma[pi]**2).rvs(5000)\n",
    "        # kl = kl_mvn((cpdf_mu[pi], scipy.stats.Covariance.from_diagonal(cpdf_sigma[pi]**2).covariance), (train_mean[pi],train_cov))\n",
    "        # print(\"KL divergence from prior to posterior: KL(posterior || prior): \",kl)\n",
    "        if plot_q_gamma:\n",
    "            phi_eh, qh = get_ellip(hypermodel)\n",
    "            phi_gh, gamma_exth = get_shear(hypermodel)\n",
    "            hypermodel[:, 1], hypermodel[:, 2], hypermodel[:, 4], hypermodel[:, 5] = gamma_exth, phi_gh, qh, phi_eh\n",
    "\n",
    "        list_of_dists.append(hypermodel)\n",
    "        categories.append(r\"cPDF: $p(\\xi_k|\\nu)$\"+\": samples from \\nprior assumed \\nfor final posteriors\")\n",
    "        colors.append(obj.color)\n",
    "        truths_list.append(mu_test[pi])\n",
    "    # list_of_dists = np.array(list_of_dists)\n",
    "    list_of_dists_new = []\n",
    "    for l in list_of_dists:\n",
    "        print(l.shape)\n",
    "        l = np.array(l)\n",
    "        list_of_dists_new.append(l[:, np.array(pi_to_plot)])\n",
    "\n",
    "    truths_list = np.array(truths_list)\n",
    "    labels_new = np.array(labels_new)\n",
    "    print(truths_list)\n",
    "    # print('list_of_dists shape: ', len(list_of_dists), len(list_of_dists[0]),len(list_of_dists[1]),len(list_of_dists[2]))\n",
    "    fig = make_contour(list_of_dists_new,labels_new[pi_to_plot],categories,colors,truths_list = [np.array(truths_list[1])[pi_to_plot]]*len(list_of_dists_new))\n",
    "    axes = np.array(fig.axes).reshape((len(pi_to_plot), len(pi_to_plot)))\n",
    "    # param_cov = scipy.stats.Covariance.from_diagonal(cpdf_sigma[pi]**2).covariance\n",
    "    # kl_list =[ukl((cpdf_mu[i], param_cov[i,i]), (train_mean[i],train_cov[i,i])) for i in pi]\n",
    "    # for i in range(len(pi)):\n",
    "    #     axes[i, i].set_title(f'IG = {np.round(kl_list[i],2)} nats', loc='left')\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f4123",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_to_plot_params = np.array([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "fig, axes = plot_interim_conditional([obj_list[0]], pi=np.array([0,1,2,3,4,5,6,7]),plot_true_dist=True,pi_to_plot=pi_to_plot_params,plot_q_gamma=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdd295",
   "metadata": {},
   "source": [
    "### Figure 11: Population inference with different mass-light correlations - Comparison to Dinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import gaussian_kde\n",
    "from latils import make_analysis_table\n",
    "from lenstronomy.Util.param_util import ellipticity2phi_q, shear_cartesian2polar\n",
    "\n",
    "# h5_files = [f'{NLobj.prep}_1300_obj.h5',f'{NLobj.prep}_no_x_y_r_m.h5',]\n",
    "params = [0,1]\n",
    "select_obj = obj_list[params]\n",
    "# h5_files = [o.h5_file for o in obj_list[params]]\n",
    "h5_files = ['data/all_lsst_0325_uniform.h5','data/all_0429_dinos_ml.h5']\n",
    "# colors = ['mediumaquamarine', 'darkgreen']\n",
    "# colors = ['gold', 'red']\n",
    "colors=['rebeccapurple','orange']\n",
    "# colors[-3:] = ['red','blue','orange']\n",
    "preps = [o.prep for o in select_obj]\n",
    "\n",
    "# preps=['nolens','nolens']\n",
    "# categories =[o.name for o in obj_list[params]]\n",
    "# categories = ['Uniform','Mass traces Light','Bright Hosts','No Distribution Shift','Informative Prior',]\n",
    "categories = [r\"Fiducial: $q_{light} \\sim q_{mass}$\", r\"Dinos 2: $q_{mass} \\geq q_{light} - 0.1$\"]\n",
    "# categories = ['All Light Included: $q_l$ < $q_m$', 'All Light Included: $q_l$ = $q_m$']\n",
    "# colors.insert(0, 'lightblue')\n",
    "# colors.insert(1, 'salmon')\n",
    "# categories = [f\"Sample {i} from True Dist\" for i in range(1,6)]\n",
    "# categories.insert(0, \"Unbiased Recovery - \\nTrue Dist Mean\")\n",
    "# categories.insert(1, 'Paltas Recovery')\n",
    "mu_test, std_test, mu_train, std_train, errors, err_perc, fig = hypermodel_plot([o.df for o in obj_list[params]], \n",
    "                      n_params_learned=[8,8],\n",
    "              preps=preps,\n",
    "              h5_files=h5_files,\n",
    "              params=np.array([0,3,4,1]),colors = colors, categories=categories,burnin=3000,\n",
    "             save_name=None, obj_list=obj_list[params]);\n",
    "# fig.tight_layout()\n",
    "axs = fig.axes \n",
    "\n",
    "axs = np.array(axs)\n",
    "for ax in axs:\n",
    "        for spine in ax.spines.values():\n",
    "                spine.set_linewidth(2)\n",
    "\n",
    "               \n",
    "axs=axs.reshape(8,8)\n",
    "all_train = pd.read_csv('data/all_train.csv')\n",
    "# all2_train = get_train_data(ALobj2.df, 'all')\n",
    "amtr,bmtr=ellipticity2phi_q(np.array([all_train['main_deflector_parameters_e1']]),np.array([all_train['main_deflector_parameters_e2']]))\n",
    "altr,bltr=ellipticity2phi_q(np.array([all_train['lens_light_parameters_e1']]),np.array([all_train['lens_light_parameters_e2']]))\n",
    "\n",
    "all = pd.read_csv('data/fiducial_test_data.csv')\n",
    "am1,bm1=ellipticity2phi_q(np.array([all['main_deflector_parameters_e1']]),np.array([all['main_deflector_parameters_e2']]))\n",
    "al2,bl1=ellipticity2phi_q(np.array([all['lens_light_parameters_e1']]),np.array([all['lens_light_parameters_e2']]))\n",
    "\n",
    "all2 = pd.read_csv('data/all_test_dinos_ellipticity.csv')\n",
    "am2,bm2=ellipticity2phi_q(np.array([all2['main_deflector_parameters_e1']]),np.array([all2['main_deflector_parameters_e2']]))\n",
    "al2,bl2=ellipticity2phi_q(np.array([all2['lens_light_parameters_e1']]),np.array([all2['lens_light_parameters_e2']]))\n",
    "\n",
    "# Create a new axis at position (2, 5) that spans the space of 4 axes\n",
    "\n",
    "# Remove the existing axis at (2, 5)\n",
    "fig.delaxes(axs[0, 5])\n",
    "\n",
    "# Add a new axis that spans the space of 4 axes\n",
    "gs = axs[2, 5].get_gridspec()\n",
    "new_ax = fig.add_subplot(gs[0:2, 5:7])\n",
    "\n",
    "# Scatter plot spanning the space of 4 axes on the figure\n",
    "# new_ax.scatter(bltr, bmtr, color='gray', label='Train', alpha=0.6)\n",
    "# Stack the arrays to shape (2, N)\n",
    "train_data = np.vstack([bltr.flatten(), bmtr.flatten()])\n",
    "bltr, bmtr = bltr.flatten(), bmtr.flatten()\n",
    "# idx = np.random.choice(len(bltr), 5000, replace=False)\n",
    "\n",
    "\n",
    "new_ax.scatter(bltr[:100000], bmtr[:100000], color='gray', alpha=0.4, label='Training Distribution')\n",
    "new_ax.scatter(bl1, bm1, color='rebeccapurple', label='Fiducial', alpha=0.6)\n",
    "new_ax.scatter(bl2, bm2, color='orange', label=r'Dinos 2: $q$ Mass-Light Relationship', alpha=0.3)\n",
    "\n",
    "# Add legend and labels\n",
    "new_ax.set_xlabel('$q_{light}$')\n",
    "new_ax.set_ylabel('$q_{mass}$')\n",
    "new_ax.set_title('Train vs Test Distribution')\n",
    "fig.suptitle(\"Population-Level Parameter Recovery\", x=0.3,y=1)\n",
    "\n",
    "nrows, ncols = axs.shape\n",
    "\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        xlabel = ax.get_xlabel()\n",
    "        ylabel = ax.get_ylabel()\n",
    "        ax.set_ylabel(ylabel, x=-0.4)\n",
    "        ax.set_xlabel(xlabel, y=-0.4)\n",
    "\n",
    "        # Left column → y ticks\n",
    "        if col == 0:\n",
    "            ax.minorticks_on()\n",
    "            ax.tick_params(axis='y', which='both', direction='out', length=6, width=1)\n",
    "        else:\n",
    "            ax.tick_params(labelleft=False, left=False)\n",
    "\n",
    "        # Bottom row → x ticks\n",
    "        if row == nrows - 1:\n",
    "            ax.minorticks_on()\n",
    "            ax.tick_params(axis='x', which='both', direction='out', length=6, width=1)\n",
    "        else:\n",
    "            ax.tick_params(labelbottom=False, bottom=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e107e1",
   "metadata": {},
   "source": [
    "### Figure 12: Relationship between bias on individual posteriors and bias in population recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5097e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chains from both files and reshape\n",
    "chain1 = retrieve_chains_h5('data/all_lsst_0325_uniform.h5')\n",
    "chain2 = retrieve_chains_h5('data/all_june10_test12.h5')\n",
    "chain1 = chain1[:, 3000:, :].reshape(-1, chain1.shape[2])\n",
    "chain2 = chain2[:, 3000:, :].reshape(-1, chain2.shape[2])\n",
    "\n",
    "# Select parameter indices to plot (example: first two parameters)\n",
    "param_indices1 = [3,11]\n",
    "param_indices2 = [0,1]\n",
    "colors=['rebeccapurple', 'mediumturquoise']\n",
    "# Prepare data for make_contour\n",
    "chains_to_plot = [chain1[:, param_indices1], chain2[:, param_indices2]]\n",
    "categories = ['Fiducial',\"Learned only $\\mathcal{M}(\\gamma_{lens})$\"]\n",
    "# Use categories and colors already defined\n",
    "mu_test = ALobj.y_test.mean(axis=0)[np.array([3])]\n",
    "std_test = ALobj.y_test.std(axis=0)[np.array([3])]\n",
    "fig = make_contour(\n",
    "    chains_to_plot,\n",
    "    labels=[mu_labels[3],std_labels[3]],\n",
    "    categories=categories,\n",
    "    colors=colors,\n",
    "    truths_list=[np.array([mu_test, std_test]).flatten(),np.array([mu_test, std_test]).flatten()]\n",
    ")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    xlabel=ax.get_xlabel()\n",
    "    ylabel=ax.get_ylabel()\n",
    "    ax.tick_params(axis='both', which='both', labelsize=40)\n",
    "    ax.set_ylabel(ylabel,fontsize=70, x=-0.2)\n",
    "    ax.set_xlabel(xlabel,fontsize=70,y=-0.2)\n",
    "    ax.minorticks_on()\n",
    "    ax.tick_params(axis='both', which='minor', length=10, width=1)\n",
    "fig.tight_layout()\n",
    "# plt.savefig(\"npe_post_to_hyper.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79a960",
   "metadata": {},
   "source": [
    "### Figure 13: Plotting the cPDF in cartesian space, displaying against the final posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bae361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_to_plot_params = np.array([0, 3, 1, 4])\n",
    "ALobj.h5_file = 'data/all_lsst_0325_uniform.h5'\n",
    "fig, axes = plot_interim_conditional([obj_list[0]], pi=np.array([0,1,2,3,4,5,6,7]),plot_true_dist=False,pi_to_plot=pi_to_plot_params,plot_q_gamma=False)\n",
    "axs = fig.axes \n",
    "axs = np.array(axs)\n",
    "               \n",
    "axs=axs.reshape(len(pi_to_plot_params), len(pi_to_plot_params))\n",
    "nrows, ncols = axs.shape\n",
    "\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        ax = axs[row, col]\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(4)\n",
    "        xlabel = ax.get_xlabel()\n",
    "        ylabel = ax.get_ylabel()\n",
    "        ax.set_ylabel(ylabel,size=40, x=-0.28)\n",
    "        ax.set_xlabel(xlabel,size=40, y=-0.3)\n",
    "        ax.xaxis.label.set_fontsize(65)\n",
    "        ax.yaxis.label.set_fontsize(65)\n",
    "\n",
    "        # Left column → y ticks\n",
    "        labelsize=40\n",
    "        if col == 0:\n",
    "            ax.minorticks_on()\n",
    "            ax.tick_params(axis='y',labelsize=labelsize, which='both', direction='out', length=6, width=1)\n",
    "            ax.yaxis.set_tick_params(labelsize=labelsize)\n",
    "        else:\n",
    "            ax.tick_params(labelleft=False, left=False)\n",
    "\n",
    "        # Bottom row → x ticks\n",
    "        if row == nrows - 1:\n",
    "            ax.minorticks_on()\n",
    "            ax.tick_params(axis='x', labelsize=labelsize,which='both', direction='out', length=6, width=1)\n",
    "            ax.xaxis.set_tick_params(labelsize=labelsize)\n",
    "\n",
    "        else:\n",
    "            ax.tick_params(labelbottom=False, bottom=False)\n",
    "\n",
    "for ax in axes[:, 1]:\n",
    "    ax.set_xlim(1.4, 2.6)\n",
    "for ax in axes[1, :1]:\n",
    "    ax.set_ylim(1.4, 2.6)\n",
    "\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_xlim(0, 4)\n",
    "for ax in axes[0, :0]:\n",
    "    ax.set_xlim(0, 4)\n",
    "\n",
    "leg = fig.legends[0]   # get the first legend in the figure\n",
    "leg.set_bbox_to_anchor((0.32, 0.8))  # adjust these numbers\n",
    "\n",
    "# fig.savefig('cPDF_fiducial_gray_purple_e1_g1.pdf',bbox_inches='tight',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
